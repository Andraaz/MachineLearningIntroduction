{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: comment choisir le meilleur modèle d'apprentissage automatique pour un problème donné?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déroulement \n",
    "- réalisation en binôme \n",
    "- A rendre : fichier jupyter + éventullement rapport d'analyse\n",
    "- Date de livraison : 30 Mars 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet nous allons traiter différents problèmes nécessitant l'utilisation des méthodes d'apprentissage automatique.\n",
    "Le but est de choisir pour un jeu de données, représentant un problème donné, le bon modèle d'apprentissage automatique compatible. Cette tâche englobe notamment le choix du meilleur pré-traitement à appliquer aux jeux de données ainsi que les bons paramètres du modèle d'apprentissage automatique.\n",
    "\n",
    "Ainsi, Résoudre le problème par le clustering revient à fournir une représentation optimale par des clusters du jeu de données associé. En classification supervisée, résoudre le problème signifie produire le modèle prédictif le plus efficace. Nous vous informons qu'un problème dont le jeu de données ne contient pas de labels est résolu globalement par les méthodes de clustering. Si les labels sont présents, dans ce cas, la classification supervisée est souhaitable. Dans un but pédagogique, nous utilisons le même ensemble labélisé de données pour la classification (supervisé) et clustering (ML non supervisé)\n",
    "\n",
    "Pour mener cette tâche, certaines étapes sont nécessaires.\n",
    "\n",
    "- La première étape donne un apperçu sur les jeux de données auxquelles nous nous intéressons. \n",
    "- Les étapes de 2 à 4 demandent à développer des fonctions nécessaires à la préparation des jeux de données. \n",
    "- A partir de l'étape 5, on procède à la recherche du modèle optimal d'apprentissage automatique pour chacun des problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet nous allons traiter différents problèmes nécessitant l'utilisation des méthodes d'apprentissage automatique.\n",
    "Le but est de choisir pour un jeu de données, représentant un problème donné, le bon modèle d'apprentissage automatique compatible. Cette tâche englobe notamment le choix du meilleur pré-traitement à appliquer aux jeux de données ainsi que les bons paramètres du modèle d'apprentissage automatique.\n",
    "\n",
    "Ainsi, Résoudre le problème par le clustering revient à fournir une représentation optimale par des clusters du jeu de données associé. En classification supervisée, résoudre le problème signifie produire le modèle prédictif le plus efficace. Nous vous informons qu'un problème dont le jeu de données ne contient pas de labels est résolu globalement par les méthodes de clustering. Si les labels sont présents, dans ce cas, la classification supervisée est souhaitable. Dans un but pédagogique, nous utilisons le même ensemble labélisé de données pour la classification (supervisé) et clustering (ML non supervisé)\n",
    "\n",
    "Pour mener cette tâche, certaines étapes sont nécessaires.\n",
    "\n",
    "- La première étape donne un apperçu sur les jeux de données auxquelles nous nous intéressons. \n",
    "- Les étapes de 2 à 4 demandent à développer des fonctions nécessaires à la préparation des jeux de données. \n",
    "- A partir de l'étape 5, on procède à la recherche du modèle optimal d'apprentissage automatique pour chacun des problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color = \"darkgreen\">1- Description des jeux de données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet, nous travaillerons sur des problèmes réels que nous cherchons à résoudre au moyen de l'apprentissage automatique :\n",
    "\n",
    "* GCM: diagnostic de cancer multiclasses (prostate, pancréans...) , chaque obervation est une représentation d'un cancer via 16063 signatures d'expression de gènes tumoraux. Le cancer pourrait avoir plusieurs representations et donc plusieurs observations.\n",
    "\n",
    "* Diabetes: diagnostic de diabètes. Une obervation représente les caractéristiques d'un patient (âge, pression, poids...) atteinte ou non du diabète.\n",
    "\n",
    "* Wall-robot-navigation: mouvements du robot SCITOS G5 naviguant dans une pièce en suivant le mur dans le sens des aiguilles d'une montre, pendant 4 tours, mesurés à l'aide de 24 capteurs à ultrasons disposés en cercle autour de sa \"taille\". Chaque obervation représente un mouvement: Avancer,  Léger virage à droite, Virage serré à droite ou Tournez légèrement à gauche (le code peut être fourni )\n",
    "\n",
    "* Japenese-vowel: cet ensemble de données enregistre 640 séries chronologiques de 12 coefficients de cepstre LPC provenant de neuf locuteurs masculins (https://fr.wikipedia.org/wiki/Cepstre). Chaque obervation représente un son d'une voyelle exprimé par un locuteur donné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">2- Chargement des jeux de données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous découpons chaque jeu de données en X et y. Pour ce faire, \n",
    "\n",
    "Développez une fonction qui charge le jeu de données depuis un fichier  et retourne deux dataframes (un pour les caractéristiques et l'autre pour la classe/labels)\n",
    "\n",
    "IL vous revient de bien analyser la structure pour determiner quelles sont les caractériqtiques et quels les calsses correspondantes (y). Néaumoins, voici quelques précisions :\n",
    "\n",
    "#### Notes : \n",
    "\n",
    "* Tous les formats sont de type csv\n",
    "* La colonne relative à la classe/label est généralement à position 0 ou à la dernière position\n",
    "* La colonne classe n'a pas le même nom dans les différents jeux de données.\n",
    "* La colonne classe n'est forcément numérique, dans ce cas la conversion s'avère nécessaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skn\n",
    "\n",
    "'''\n",
    "def read_csv_parse_labels(path: str, isFirstColumn=False):\n",
    "  df = pd.read_csv(path, low_memory=False)\n",
    "  y = None\n",
    "  if isFirstColumn:\n",
    "    y = df.iloc[:, 0]\n",
    "  else:\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "  labels = y.unique()\n",
    "  mapping = dict(zip(labels, range(len(labels))))\n",
    "  y = y.map(mapping)\n",
    "  df.pop(y.name)\n",
    "  return df, y, labels\n",
    "'''\n",
    "\n",
    "def read_df_parse_labels(df, isFirstColumn=False):\n",
    "  y = None\n",
    "  if isFirstColumn:\n",
    "    y = df.iloc[:, 0]\n",
    "  else:\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "  labels = y.unique()\n",
    "  mapping = dict(zip(labels, range(len(labels))))\n",
    "  y = y.map(mapping)\n",
    "  df.pop(y.name)\n",
    "  return df, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guillaume\\AppData\\Local\\Temp\\ipykernel_16260\\247894383.py:2: DtypeWarning: Columns (1,2,4,5,7,8,10,11,12,13,14,16,17,20,21,23,24,26,27,28,29,30,31,32,33,35,36,39,40,42,48,54,55,57,63,64,67,68,69,70,71,73,74,81,83,95,99,101,104,105,108,110,113,114,117,118,122,124,125,133,140,141,144,146,159,161,162,164,167,176,177,178,184,186,187,188,191,192,194,198,204,206,212,213,218,219,224,226,230,235,236,240,241,245,250,255,256,257,259,264,265,266,268,270,271,273,277,278,285,294,302,319,325,329,331,334,340,341,343,348,353,354,366,371,376,382,386,387,388,390,391,393,396,407,408,409,412,413,416,417,418,419,420,421,428,429,438,443,447,454,455,456,459,467,468,470,471,477,484,485,486,490,492,493,503,510,514,516,522,524,545,547,550,556,557,559,561,564,565,569,571,572,573,574,582,585,589,595,596,597,598,600,608,618,620,623,625,626,631,633,636,637,638,643,644,646,650,652,656,657,658,661,664,669,673,674,679,680,683,684,685,687,690,691,692,693,694,696,697,699,703,704,711,713,724,726,727,730,732,734,741,749,751,754,757,760,761,763,768,769,773,776,779,781,784,786,789,791,795,800,802,808,809,810,812,821,822,824,825,826,827,828,829,830,840,842,843,848,849,851,855,856,858,859,862,863,864,865,868,870,871,883,887,888,891,898,901,904,906,908,909,912,919,922,924,926,928,931,935,936,937,938,939,940,942,943,944,945,946,949,950,951,953,956,957,958,961,962,966,969,975,976,977,982,983,984,985,988,991,993,994,998,1000,1003,1004,1005,1006,1011,1013,1015,1016,1017,1018,1019,1022,1026,1028,1029,1030,1032,1036,1037,1038,1040,1042,1044,1047,1050,1052,1058,1066,1068,1074,1078,1079,1086,1088,1090,1091,1092,1094,1097,1098,1099,1101,1102,1105,1118,1121,1122,1123,1128,1134,1135,1137,1138,1141,1142,1143,1147,1150,1159,1161,1163,1164,1166,1167,1171,1174,1181,1182,1183,1184,1185,1189,1190,1192,1198,1203,1205,1206,1207,1209,1211,1212,1213,1214,1217,1218,1220,1230,1235,1236,1237,1239,1240,1242,1244,1246,1249,1251,1258,1260,1261,1262,1268,1270,1271,1272,1274,1275,1280,1282,1283,1284,1289,1291,1292,1301,1304,1305,1307,1310,1312,1313,1322,1325,1335,1336,1337,1338,1339,1340,1343,1347,1350,1366,1373,1378,1384,1394,1395,1400,1401,1408,1411,1412,1414,1415,1418,1422,1424,1425,1430,1434,1437,1438,1439,1442,1443,1445,1448,1449,1453,1456,1457,1460,1464,1470,1471,1473,1474,1476,1484,1485,1488,1491,1492,1493,1494,1502,1503,1504,1505,1506,1508,1511,1514,1515,1516,1519,1521,1523,1524,1525,1526,1528,1531,1532,1536,1542,1547,1552,1553,1554,1555,1557,1558,1559,1563,1565,1569,1570,1573,1577,1581,1582,1584,1588,1589,1590,1592,1593,1594,1596,1599,1600,1601,1603,1605,1609,1614,1617,1625,1628,1629,1631,1634,1639,1645,1646,1647,1654,1655,1662,1664,1666,1670,1675,1677,1678,1679,1681,1686,1687,1688,1689,1690,1691,1696,1698,1701,1705,1708,1711,1715,1717,1718,1721,1722,1725,1726,1734,1735,1737,1739,1742,1744,1747,1748,1750,1751,1753,1754,1756,1757,1767,1768,1771,1776,1778,1780,1785,1786,1791,1793,1794,1796,1798,1804,1805,1806,1810,1812,1814,1815,1816,1818,1821,1823,1832,1837,1839,1842,1843,1844,1845,1847,1849,1851,1852,1857,1861,1862,1864,1865,1868,1872,1873,1874,1878,1879,1880,1883,1886,1887,1888,1893,1895,1897,1901,1905,1906,1910,1912,1914,1922,1924,1925,1926,1933,1935,1936,1937,1938,1939,1942,1945,1946,1947,1949,1950,1955,1957,1960,1963,1965,1967,1972,1975,1984,1988,1996,1999,2000,2005,2006,2010,2013,2016,2017,2021,2026,2027,2029,2033,2035,2036,2038,2039,2043,2044,2045,2046,2049,2053,2054,2056,2058,2066,2069,2070,2071,2073,2077,2079,2081,2085,2088,2089,2090,2091,2096,2098,2100,2106,2112,2113,2116,2117,2118,2125,2126,2128,2130,2131,2132,2136,2139,2141,2143,2149,2151,2153,2154,2155,2157,2159,2160,2162,2166,2169,2173,2174,2175,2177,2179,2181,2188,2190,2191,2192,2193,2197,2199,2200,2201,2202,2203,2205,2206,2207,2208,2211,2212,2217,2220,2223,2224,2225,2230,2231,2232,2233,2237,2241,2244,2245,2249,2253,2254,2257,2260,2261,2265,2266,2270,2273,2275,2278,2279,2280,2282,2284,2290,2291,2294,2295,2296,2300,2301,2306,2308,2311,2312,2313,2320,2321,2322,2326,2327,2333,2334,2335,2336,2340,2341,2345,2346,2350,2351,2352,2353,2355,2356,2357,2358,2362,2365,2372,2375,2377,2378,2381,2384,2386,2387,2388,2398,2399,2401,2402,2406,2411,2414,2415,2420,2422,2424,2429,2430,2433,2434,2439,2443,2445,2446,2454,2465,2467,2469,2470,2471,2472,2475,2480,2482,2485,2487,2489,2491,2494,2496,2499,2501,2504,2516,2517,2519,2522,2523,2525,2526,2529,2530,2531,2534,2538,2539,2542,2544,2546,2551,2552,2554,2555,2557,2560,2565,2570,2572,2573,2576,2577,2583,2584,2587,2588,2592,2606,2609,2612,2614,2625,2628,2629,2631,2632,2633,2636,2637,2639,2643,2645,2649,2651,2652,2659,2662,2668,2675,2678,2680,2682,2684,2686,2690,2691,2694,2698,2699,2703,2708,2711,2714,2716,2717,2720,2723,2725,2730,2731,2732,2735,2737,2740,2746,2750,2762,2766,2771,2772,2774,2776,2779,2780,2786,2788,2799,2805,2810,2811,2813,2814,2816,2820,2822,2825,2826,2830,2831,2833,2834,2835,2836,2837,2839,2840,2843,2849,2851,2853,2855,2856,2857,2858,2859,2868,2869,2870,2871,2873,2877,2880,2882,2883,2890,2895,2896,2898,2901,2907,2908,2910,2916,2921,2922,2923,2925,2927,2930,2932,2936,2937,2939,2940,2941,2943,2953,2954,2956,2961,2962,2966,2967,2968,2970,2971,2972,2975,2976,2979,2982,2983,2985,2991,2996,2999,3000,3006,3008,3011,3012,3014,3015,3018,3020,3023,3024,3025,3027,3034,3035,3036,3038,3042,3043,3045,3050,3052,3056,3057,3064,3066,3069,3070,3072,3073,3077,3079,3082,3084,3085,3087,3090,3091,3092,3093,3094,3097,3099,3100,3101,3102,3103,3114,3115,3117,3118,3119,3120,3124,3130,3132,3133,3135,3139,3141,3142,3143,3144,3150,3152,3154,3155,3157,3158,3159,3163,3165,3166,3172,3178,3180,3185,3186,3187,3188,3189,3191,3192,3194,3196,3200,3201,3202,3209,3210,3212,3213,3215,3216,3217,3219,3223,3224,3237,3244,3249,3261,3263,3270,3273,3275,3277,3279,3286,3287,3288,3290,3292,3297,3298,3299,3300,3302,3303,3304,3305,3316,3318,3323,3326,3329,3331,3334,3339,3342,3343,3351,3361,3362,3367,3372,3379,3380,3381,3382,3383,3385,3387,3388,3394,3399,3400,3406,3407,3411,3413,3415,3416,3417,3418,3419,3422,3424,3426,3433,3436,3437,3441,3442,3446,3454,3456,3457,3458,3465,3467,3481,3483,3485,3487,3489,3490,3493,3495,3498,3500,3501,3511,3512,3513,3515,3517,3518,3521,3523,3524,3525,3526,3527,3533,3534,3535,3536,3539,3540,3542,3543,3546,3549,3550,3552,3554,3555,3562,3567,3571,3581,3586,3594,3600,3601,3605,3607,3608,3609,3612,3614,3615,3619,3620,3621,3622,3623,3624,3631,3634,3635,3639,3647,3650,3652,3655,3659,3660,3661,3664,3669,3671,3673,3676,3677,3678,3679,3680,3682,3683,3686,3687,3688,3690,3692,3694,3695,3696,3702,3703,3708,3710,3711,3714,3718,3720,3722,3723,3724,3730,3731,3732,3734,3735,3736,3744,3745,3746,3748,3750,3752,3755,3767,3770,3774,3775,3778,3782,3783,3784,3790,3791,3792,3793,3795,3796,3798,3800,3801,3805,3808,3809,3810,3813,3815,3816,3818,3819,3825,3826,3828,3831,3836,3837,3838,3839,3840,3842,3844,3848,3849,3853,3859,3862,3863,3864,3870,3872,3875,3878,3881,3886,3889,3890,3894,3895,3898,3899,3901,3904,3910,3912,3916,3917,3918,3919,3920,3922,3923,3926,3931,3933,3937,3941,3944,3946,3950,3951,3952,3954,3955,3956,3959,3965,3966,3968,3970,3972,3975,3977,3981,3982,3988,3993,3995,3996,3998,4001,4006,4010,4012,4015,4021,4022,4023,4028,4031,4032,4033,4046,4047,4048,4053,4055,4056,4057,4062,4065,4066,4068,4069,4070,4072,4073,4075,4076,4080,4082,4084,4085,4087,4088,4090,4097,4098,4099,4100,4101,4102,4104,4105,4106,4108,4109,4113,4120,4121,4122,4125,4126,4130,4133,4134,4136,4143,4153,4154,4156,4157,4163,4164,4168,4169,4171,4179,4188,4189,4190,4191,4192,4196,4197,4200,4203,4205,4207,4208,4211,4212,4213,4215,4216,4218,4219,4220,4222,4223,4226,4228,4229,4230,4232,4237,4240,4248,4250,4251,4252,4254,4256,4257,4258,4260,4262,4264,4265,4268,4275,4276,4278,4280,4281,4282,4286,4289,4292,4294,4295,4299,4306,4310,4312,4315,4319,4320,4322,4323,4326,4329,4330,4332,4334,4337,4338,4346,4348,4354,4355,4357,4359,4363,4371,4378,4379,4384,4386,4400,4402,4403,4406,4407,4408,4409,4413,4414,4416,4417,4419,4420,4425,4426,4427,4431,4434,4442,4444,4448,4449,4450,4454,4455,4456,4457,4461,4463,4465,4475,4477,4478,4499,4501,4502,4507,4508,4511,4514,4517,4521,4522,4528,4529,4536,4537,4540,4547,4560,4561,4567,4568,4569,4571,4573,4575,4577,4578,4583,4586,4590,4594,4595,4596,4602,4603,4606,4608,4609,4617,4625,4626,4627,4628,4630,4631,4633,4639,4644,4660,4661,4666,4670,4671,4674,4678,4680,4683,4684,4686,4690,4692,4700,4701,4703,4707,4708,4710,4712,4713,4715,4717,4718,4719,4720,4722,4724,4725,4726,4732,4733,4734,4735,4736,4742,4743,4744,4747,4749,4750,4751,4754,4755,4756,4760,4764,4765,4772,4775,4776,4777,4781,4788,4792,4794,4797,4798,4803,4805,4806,4811,4812,4813,4814,4815,4817,4819,4821,4824,4826,4827,4829,4831,4832,4833,4838,4842,4843,4844,4845,4847,4857,4862,4868,4870,4871,4872,4875,4876,4877,4878,4882,4883,4884,4885,4889,4890,4892,4894,4897,4901,4902,4904,4906,4908,4909,4910,4913,4914,4915,4919,4923,4925,4926,4929,4932,4933,4937,4939,4944,4953,4954,4956,4957,4959,4962,4965,4966,4968,4969,4970,4971,4973,4977,4984,4985,4989,4994,4995,4998,4999,5000,5001,5002,5003,5004,5005,5010,5012,5014,5015,5016,5019,5021,5023,5024,5028,5031,5035,5036,5037,5038,5039,5046,5048,5050,5052,5053,5055,5059,5061,5063,5067,5068,5069,5070,5071,5072,5075,5077,5083,5084,5097,5098,5099,5100,5103,5107,5109,5113,5114,5117,5118,5123,5126,5127,5128,5132,5134,5135,5140,5143,5154,5156,5157,5159,5161,5165,5167,5170,5171,5172,5177,5184,5187,5188,5192,5194,5195,5197,5199,5202,5203,5204,5205,5208,5209,5210,5211,5212,5214,5215,5217,5218,5220,5221,5222,5224,5225,5227,5229,5232,5235,5240,5246,5248,5250,5256,5257,5260,5266,5268,5275,5278,5279,5285,5294,5304,5312,5315,5321,5325,5326,5333,5334,5347,5353,5355,5357,5359,5367,5370,5371,5372,5376,5377,5378,5379,5380,5386,5388,5391,5392,5393,5394,5395,5397,5398,5403,5407,5410,5412,5416,5418,5422,5423,5424,5425,5428,5429,5431,5432,5433,5434,5436,5437,5445,5448,5452,5453,5455,5456,5458,5459,5460,5465,5470,5475,5480,5481,5489,5490,5491,5493,5495,5498,5499,5502,5503,5507,5508,5510,5511,5512,5514,5515,5523,5525,5526,5527,5533,5537,5545,5546,5549,5551,5552,5560,5563,5567,5569,5571,5575,5576,5577,5581,5584,5585,5589,5591,5594,5595,5596,5597,5598,5599,5600,5601,5602,5606,5609,5610,5612,5614,5615,5621,5622,5626,5627,5634,5637,5639,5655,5658,5668,5669,5671,5674,5679,5680,5681,5684,5685,5690,5691,5694,5695,5697,5698,5699,5700,5705,5708,5710,5711,5720,5721,5723,5730,5731,5733,5734,5735,5736,5742,5743,5744,5746,5748,5751,5752,5753,5756,5757,5758,5759,5763,5764,5765,5766,5768,5769,5770,5771,5773,5775,5776,5783,5784,5787,5789,5790,5791,5800,5803,5807,5808,5810,5812,5813,5826,5827,5830,5832,5838,5840,5853,5855,5856,5859,5862,5865,5866,5867,5868,5869,5875,5876,5879,5880,5888,5891,5893,5894,5896,5899,5901,5903,5905,5908,5911,5912,5914,5915,5918,5919,5924,5926,5928,5931,5933,5935,5940,5941,5943,5945,5951,5952,5953,5957,5959,5961,5962,5964,5965,5969,5973,5976,5980,5987,5997,5998,6001,6002,6003,6005,6007,6010,6011,6015,6016,6020,6023,6026,6027,6028,6030,6033,6037,6043,6044,6045,6047,6048,6051,6053,6055,6061,6062,6064,6067,6068,6069,6070,6076,6077,6081,6082,6083,6084,6089,6090,6092,6096,6097,6101,6103,6108,6110,6114,6117,6120,6124,6126,6127,6128,6134,6141,6144,6145,6146,6147,6148,6149,6150,6151,6154,6159,6162,6164,6167,6169,6173,6174,6176,6181,6182,6184,6186,6188,6190,6192,6193,6194,6197,6198,6199,6201,6202,6203,6209,6210,6213,6215,6216,6218,6219,6220,6227,6228,6230,6231,6234,6236,6239,6240,6244,6245,6251,6253,6254,6255,6256,6257,6262,6263,6265,6266,6271,6273,6275,6278,6281,6285,6286,6288,6290,6291,6293,6295,6298,6301,6302,6305,6311,6312,6315,6316,6322,6329,6332,6335,6338,6342,6343,6344,6346,6348,6350,6353,6354,6355,6359,6360,6362,6363,6367,6369,6370,6371,6384,6385,6387,6389,6392,6396,6399,6403,6404,6405,6407,6408,6409,6411,6416,6419,6420,6422,6429,6431,6435,6436,6445,6448,6451,6453,6455,6458,6459,6460,6468,6469,6470,6478,6479,6480,6481,6482,6484,6486,6487,6491,6496,6497,6498,6503,6504,6507,6509,6512,6517,6518,6521,6522,6523,6529,6530,6532,6535,6537,6540,6541,6542,6543,6544,6545,6549,6551,6553,6559,6562,6564,6565,6569,6571,6576,6579,6583,6586,6587,6590,6591,6593,6598,6600,6602,6604,6605,6613,6616,6621,6627,6628,6629,6630,6631,6632,6633,6634,6636,6639,6641,6643,6645,6647,6649,6650,6651,6652,6654,6656,6657,6661,6662,6663,6666,6667,6672,6678,6679,6682,6683,6686,6689,6690,6693,6694,6695,6696,6697,6701,6704,6705,6706,6707,6709,6711,6713,6714,6715,6722,6725,6731,6734,6735,6736,6739,6741,6742,6746,6747,6749,6751,6752,6754,6755,6756,6757,6758,6761,6762,6767,6769,6771,6774,6775,6776,6777,6781,6782,6783,6784,6786,6791,6797,6798,6800,6803,6808,6809,6817,6818,6819,6823,6831,6832,6833,6839,6840,6841,6842,6844,6848,6850,6852,6859,6861,6863,6864,6865,6867,6869,6870,6874,6875,6878,6881,6882,6885,6886,6887,6890,6891,6892,6898,6906,6908,6912,6916,6920,6921,6922,6923,6930,6932,6934,6937,6939,6940,6945,6955,6957,6959,6960,6961,6962,6963,6964,6967,6969,6973,6977,6979,6980,6982,6985,6989,6990,6992,6993,6994,6995,6997,6999,7004,7006,7007,7010,7011,7016,7020,7021,7024,7027,7028,7030,7032,7033,7034,7038,7043,7045,7046,7048,7050,7051,7052,7053,7059,7066,7067,7073,7074,7077,7079,7081,7082,7083,7085,7087,7089,7091,7092,7094,7099,7104,7105,7107,7109,7111,7113,7119,7121,7123,7124,7125,7126,7128,7130,7131,7134,7135,7136,7139,7140,7142,7143,7145,7146,7148,7149,7150,7151,7152,7154,7155,7160,7161,7163,7164,7166,7167,7168,7169,7170,7171,7172,7173,7175,7176,7179,7180,7182,7188,7194,7195,7197,7201,7203,7210,7211,7212,7215,7217,7221,7222,7232,7234,7239,7240,7242,7245,7246,7250,7251,7255,7256,7258,7259,7267,7269,7272,7276,7279,7283,7293,7294,7296,7298,7300,7301,7304,7307,7310,7312,7313,7314,7316,7320,7326,7332,7333,7343,7346,7350,7357,7358,7361,7363,7365,7367,7368,7369,7370,7372,7375,7376,7380,7381,7382,7386,7388,7392,7393,7394,7397,7399,7403,7404,7406,7409,7414,7416,7420,7425,7428,7430,7431,7433,7434,7439,7441,7445,7448,7451,7452,7453,7457,7458,7460,7462,7464,7465,7468,7470,7472,7478,7479,7480,7484,7486,7487,7488,7491,7493,7494,7497,7499,7508,7511,7513,7519,7520,7523,7524,7525,7532,7540,7542,7543,7544,7547,7550,7551,7554,7555,7560,7567,7574,7576,7577,7578,7581,7583,7585,7586,7587,7591,7593,7596,7597,7598,7601,7603,7605,7607,7609,7611,7613,7617,7618,7621,7629,7630,7631,7632,7635,7636,7638,7640,7648,7649,7651,7653,7655,7656,7657,7660,7661,7662,7669,7670,7672,7676,7678,7689,7690,7691,7695,7700,7706,7712,7713,7719,7723,7727,7728,7729,7733,7734,7736,7737,7739,7741,7747,7748,7753,7756,7758,7759,7760,7761,7762,7766,7767,7768,7770,7771,7772,7773,7774,7776,7777,7778,7779,7780,7781,7782,7783,7784,7786,7788,7790,7792,7796,7798,7800,7803,7804,7807,7808,7809,7813,7814,7815,7818,7821,7828,7832,7837,7841,7846,7850,7851,7854,7855,7856,7863,7866,7869,7874,7880,7882,7883,7884,7886,7887,7895,7896,7904,7910,7913,7917,7927,7930,7931,7933,7937,7938,7940,7941,7942,7944,7945,7946,7950,7951,7957,7963,7965,7969,7970,7972,7973,7979,7984,7988,7992,7995,7999,8002,8006,8009,8014,8015,8021,8022,8023,8025,8029,8031,8032,8034,8039,8041,8044,8045,8048,8049,8052,8055,8056,8057,8064,8065,8066,8067,8069,8070,8073,8078,8080,8082,8083,8085,8087,8089,8092,8098,8100,8102,8103,8110,8112,8113,8114,8118,8120,8124,8126,8130,8131,8133,8136,8137,8138,8142,8143,8147,8153,8157,8162,8167,8169,8171,8172,8173,8175,8178,8179,8181,8186,8195,8201,8202,8205,8206,8207,8211,8216,8222,8224,8225,8226,8227,8228,8229,8230,8231,8233,8234,8239,8240,8241,8244,8245,8251,8252,8255,8256,8258,8259,8261,8263,8265,8267,8269,8270,8271,8274,8275,8276,8278,8279,8280,8282,8283,8287,8289,8292,8295,8296,8298,8299,8302,8304,8307,8308,8311,8314,8315,8316,8319,8322,8327,8328,8329,8330,8333,8339,8340,8341,8342,8343,8353,8354,8355,8356,8357,8360,8361,8362,8363,8365,8366,8367,8368,8372,8373,8374,8379,8380,8381,8383,8387,8389,8392,8393,8396,8397,8401,8405,8406,8412,8414,8416,8419,8421,8422,8427,8430,8436,8440,8442,8444,8446,8448,8449,8450,8452,8453,8456,8459,8460,8463,8466,8469,8472,8473,8474,8478,8482,8485,8490,8499,8504,8505,8506,8507,8510,8515,8516,8518,8519,8520,8525,8527,8528,8529,8530,8533,8539,8540,8541,8542,8543,8545,8551,8554,8557,8558,8560,8563,8565,8567,8568,8573,8574,8577,8578,8581,8583,8585,8588,8591,8595,8608,8609,8610,8611,8612,8616,8617,8620,8621,8622,8623,8624,8628,8635,8636,8637,8638,8640,8642,8643,8646,8647,8649,8650,8652,8655,8656,8659,8660,8662,8664,8666,8673,8681,8683,8684,8686,8692,8696,8697,8701,8702,8703,8704,8707,8708,8709,8710,8713,8715,8716,8721,8727,8728,8734,8735,8741,8747,8750,8754,8755,8756,8759,8765,8768,8770,8776,8778,8784,8785,8793,8794,8795,8796,8799,8803,8806,8808,8810,8811,8812,8814,8815,8816,8822,8825,8828,8829,8830,8831,8832,8833,8834,8835,8836,8841,8844,8846,8849,8850,8851,8852,8854,8855,8861,8865,8868,8869,8872,8873,8875,8881,8882,8883,8885,8886,8888,8894,8895,8899,8900,8901,8904,8906,8907,8908,8914,8915,8916,8917,8918,8921,8923,8926,8927,8928,8929,8932,8933,8934,8935,8936,8938,8941,8946,8951,8956,8957,8964,8965,8966,8967,8971,8972,8976,8983,8987,8989,8990,8991,8994,8995,9003,9004,9007,9014,9015,9019,9021,9022,9026,9027,9028,9029,9034,9041,9048,9050,9051,9054,9055,9057,9058,9060,9062,9064,9066,9068,9070,9072,9074,9075,9085,9088,9096,9097,9098,9099,9102,9103,9107,9110,9112,9114,9115,9117,9118,9129,9135,9139,9142,9144,9147,9148,9149,9152,9154,9156,9159,9160,9161,9162,9167,9170,9177,9180,9181,9183,9184,9186,9188,9189,9193,9195,9198,9199,9201,9202,9204,9210,9213,9216,9219,9220,9224,9228,9229,9232,9236,9241,9250,9253,9254,9255,9270,9278,9284,9285,9292,9297,9300,9309,9314,9319,9320,9321,9322,9324,9330,9331,9335,9337,9338,9340,9342,9343,9344,9347,9348,9349,9350,9351,9355,9357,9361,9362,9363,9364,9365,9368,9369,9371,9372,9374,9378,9386,9391,9392,9394,9398,9400,9404,9405,9409,9411,9424,9425,9427,9434,9436,9437,9438,9439,9440,9442,9446,9449,9450,9451,9455,9456,9457,9458,9461,9462,9464,9468,9469,9470,9474,9475,9476,9477,9479,9480,9481,9483,9485,9488,9489,9490,9491,9492,9500,9503,9505,9507,9513,9516,9517,9522,9528,9529,9530,9531,9532,9534,9536,9537,9540,9542,9544,9548,9550,9554,9557,9562,9563,9566,9567,9568,9570,9575,9579,9582,9583,9585,9603,9604,9605,9606,9607,9610,9615,9618,9620,9621,9623,9625,9637,9638,9649,9652,9656,9658,9659,9663,9666,9672,9673,9674,9675,9677,9679,9681,9682,9684,9685,9687,9688,9689,9691,9695,9697,9699,9701,9704,9706,9708,9709,9712,9713,9714,9715,9716,9718,9719,9721,9722,9724,9725,9727,9728,9746,9747,9748,9749,9751,9752,9757,9758,9759,9762,9766,9769,9775,9781,9789,9791,9794,9796,9800,9804,9806,9807,9808,9809,9810,9813,9814,9816,9818,9819,9820,9821,9823,9824,9826,9829,9833,9834,9835,9836,9837,9838,9839,9840,9841,9843,9846,9849,9851,9856,9857,9861,9862,9870,9873,9874,9875,9877,9878,9881,9885,9888,9889,9890,9891,9896,9898,9900,9901,9902,9904,9905,9908,9909,9910,9912,9913,9914,9915,9916,9928,9930,9931,9933,9934,9935,9937,9938,9940,9945,9947,9948,9949,9950,9951,9952,9954,9957,9958,9960,9968,9970,9973,9974,9975,9976,9977,9979,9980,9981,9988,9991,9993,9996,10001,10002,10005,10007,10010,10011,10016,10017,10018,10019,10020,10022,10028,10030,10031,10036,10040,10041,10043,10044,10045,10050,10057,10058,10060,10061,10064,10068,10071,10072,10074,10076,10078,10085,10087,10095,10097,10099,10100,10106,10108,10115,10117,10118,10119,10123,10124,10125,10128,10129,10130,10131,10135,10137,10140,10143,10144,10145,10148,10150,10151,10154,10156,10161,10163,10168,10170,10172,10177,10184,10193,10194,10202,10204,10208,10212,10214,10221,10223,10224,10225,10227,10230,10231,10232,10233,10239,10245,10246,10248,10251,10262,10264,10267,10270,10273,10280,10281,10282,10284,10286,10287,10288,10290,10291,10295,10296,10303,10304,10306,10307,10309,10310,10313,10319,10320,10322,10323,10325,10326,10327,10328,10330,10332,10333,10336,10338,10339,10340,10342,10348,10349,10351,10352,10353,10360,10361,10363,10364,10365,10369,10370,10371,10376,10380,10381,10384,10385,10387,10388,10395,10397,10398,10400,10404,10406,10407,10410,10415,10416,10417,10419,10420,10422,10424,10426,10427,10428,10429,10430,10435,10440,10442,10447,10452,10453,10456,10465,10467,10470,10472,10474,10475,10477,10482,10484,10485,10490,10493,10494,10495,10500,10506,10507,10510,10511,10514,10516,10517,10518,10521,10526,10527,10529,10531,10532,10533,10535,10537,10538,10540,10541,10544,10545,10546,10547,10550,10551,10554,10555,10557,10564,10565,10566,10567,10568,10576,10579,10580,10581,10582,10583,10585,10589,10594,10596,10605,10612,10613,10614,10618,10622,10623,10625,10626,10627,10628,10631,10636,10637,10638,10639,10641,10643,10646,10649,10650,10657,10658,10661,10664,10665,10669,10671,10672,10673,10674,10675,10676,10677,10678,10679,10684,10685,10686,10692,10694,10701,10703,10705,10706,10707,10710,10713,10718,10719,10721,10726,10729,10730,10732,10734,10738,10749,10750,10751,10754,10757,10758,10761,10765,10767,10769,10772,10774,10777,10781,10782,10783,10784,10789,10791,10794,10795,10796,10797,10798,10799,10802,10807,10809,10810,10811,10816,10817,10820,10822,10825,10827,10831,10833,10836,10837,10841,10843,10845,10847,10852,10854,10856,10858,10860,10863,10864,10865,10868,10871,10872,10876,10877,10881,10885,10886,10887,10889,10890,10893,10898,10900,10901,10902,10903,10906,10911,10912,10913,10916,10919,10921,10922,10924,10926,10927,10928,10929,10939,10940,10941,10943,10944,10946,10956,10959,10960,10962,10963,10965,10969,10971,10972,10974,10975,10979,10980,10981,10984,10986,10989,10991,10992,10994,10997,11011,11014,11018,11019,11020,11021,11025,11027,11034,11035,11038,11040,11042,11044,11045,11046,11049,11056,11057,11058,11059,11062,11063,11066,11068,11069,11071,11073,11076,11077,11078,11079,11083,11086,11088,11089,11090,11093,11094,11095,11099,11100,11103,11107,11108,11109,11110,11112,11113,11114,11119,11120,11121,11124,11129,11131,11132,11133,11134,11135,11137,11138,11141,11143,11145,11152,11153,11154,11156,11165,11170,11171,11174,11175,11176,11177,11179,11181,11182,11184,11187,11189,11191,11192,11195,11198,11199,11200,11203,11206,11208,11209,11211,11216,11217,11218,11222,11223,11224,11228,11230,11233,11236,11239,11240,11241,11242,11244,11246,11250,11251,11253,11256,11261,11268,11269,11273,11280,11287,11292,11295,11300,11301,11315,11319,11326,11327,11328,11333,11334,11335,11336,11338,11341,11343,11349,11352,11356,11358,11360,11361,11367,11378,11379,11381,11382,11387,11389,11390,11391,11392,11393,11396,11401,11402,11404,11406,11408,11412,11417,11418,11419,11421,11422,11430,11433,11434,11435,11436,11439,11440,11441,11442,11445,11447,11448,11449,11450,11451,11454,11457,11462,11465,11468,11470,11473,11476,11477,11484,11488,11492,11495,11496,11498,11500,11503,11511,11513,11515,11518,11519,11521,11522,11524,11525,11526,11528,11530,11532,11534,11535,11536,11537,11538,11539,11540,11541,11545,11546,11549,11550,11551,11552,11553,11559,11565,11569,11572,11575,11576,11579,11580,11581,11583,11585,11586,11589,11592,11597,11599,11600,11605,11610,11613,11614,11615,11616,11617,11619,11623,11626,11629,11633,11634,11635,11642,11645,11648,11654,11656,11658,11662,11664,11666,11674,11681,11683,11691,11694,11695,11697,11703,11704,11706,11711,11716,11718,11720,11722,11729,11730,11733,11736,11737,11739,11740,11743,11744,11748,11750,11754,11758,11760,11761,11762,11763,11765,11766,11767,11769,11772,11775,11782,11786,11791,11795,11804,11805,11807,11808,11810,11811,11814,11818,11819,11821,11824,11831,11833,11834,11836,11837,11842,11848,11850,11854,11856,11857,11860,11865,11873,11874,11880,11881,11887,11894,11895,11897,11898,11899,11901,11908,11909,11912,11913,11915,11916,11917,11918,11921,11928,11929,11930,11932,11938,11941,11942,11945,11946,11947,11952,11954,11955,11956,11964,11966,11976,11980,11981,11982,11983,11993,11996,11997,11998,11999,12002,12006,12009,12012,12016,12018,12024,12025,12026,12027,12032,12033,12036,12039,12042,12044,12045,12050,12053,12054,12056,12059,12060,12071,12072,12073,12074,12080,12084,12086,12091,12093,12098,12103,12106,12123,12127,12130,12132,12141,12148,12157,12159,12162,12163,12166,12167,12169,12173,12175,12176,12178,12181,12189,12190,12191,12194,12198,12208,12209,12210,12212,12219,12221,12222,12226,12228,12231,12232,12241,12244,12246,12251,12253,12256,12257,12258,12260,12262,12263,12264,12265,12266,12267,12274,12276,12281,12282,12284,12288,12289,12290,12292,12293,12294,12296,12298,12301,12308,12313,12317,12318,12319,12326,12332,12333,12335,12338,12340,12344,12346,12347,12351,12358,12360,12362,12367,12369,12371,12377,12385,12389,12390,12392,12395,12398,12401,12403,12406,12408,12409,12412,12418,12419,12421,12422,12423,12425,12426,12427,12428,12429,12430,12435,12436,12437,12440,12441,12443,12445,12446,12452,12456,12458,12460,12469,12471,12472,12473,12475,12476,12480,12492,12493,12496,12498,12499,12501,12502,12504,12505,12506,12509,12515,12518,12519,12527,12531,12534,12538,12543,12545,12546,12555,12558,12560,12564,12567,12570,12571,12573,12574,12577,12580,12581,12584,12586,12587,12588,12591,12595,12598,12600,12603,12604,12607,12609,12610,12613,12614,12624,12625,12629,12631,12632,12636,12637,12638,12640,12642,12646,12650,12651,12663,12667,12668,12669,12672,12676,12677,12682,12686,12687,12688,12690,12692,12693,12696,12702,12710,12712,12714,12718,12721,12723,12726,12728,12729,12730,12731,12732,12734,12745,12756,12758,12760,12763,12766,12770,12777,12786,12787,12799,12800,12809,12811,12823,12833,12834,12835,12837,12840,12849,12851,12853,12854,12855,12856,12866,12869,12871,12880,12891,12892,12894,12901,12902,12903,12909,12911,12913,12916,12918,12919,12920,12921,12923,12924,12927,12931,12934,12939,12941,12943,12947,12948,12950,12952,12958,12962,12963,12967,12968,12970,12976,12991,12995,12996,12998,12999,13000,13002,13008,13009,13012,13015,13018,13019,13021,13022,13025,13027,13028,13030,13032,13039,13044,13051,13055,13056,13057,13060,13067,13068,13069,13073,13085,13089,13090,13094,13100,13107,13108,13111,13113,13115,13117,13119,13122,13126,13130,13134,13140,13145,13146,13152,13157,13161,13162,13168,13170,13173,13174,13177,13178,13180,13181,13189,13190,13195,13197,13206,13207,13212,13218,13219,13220,13221,13237,13238,13239,13250,13252,13253,13256,13260,13267,13273,13275,13277,13279,13280,13282,13285,13286,13289,13294,13296,13301,13305,13308,13309,13314,13318,13326,13332,13333,13337,13344,13347,13353,13356,13357,13360,13361,13365,13366,13369,13376,13377,13381,13383,13385,13389,13390,13393,13394,13396,13399,13400,13409,13410,13411,13417,13422,13423,13425,13431,13433,13435,13437,13439,13441,13446,13449,13466,13467,13474,13476,13479,13481,13482,13491,13492,13496,13497,13502,13508,13509,13511,13512,13513,13515,13517,13519,13522,13524,13527,13528,13529,13532,13533,13536,13537,13538,13541,13543,13544,13549,13553,13554,13556,13557,13570,13571,13573,13574,13587,13591,13598,13599,13603,13618,13628,13629,13630,13637,13640,13645,13651,13654,13656,13674,13675,13678,13679,13681,13685,13686,13689,13690,13692,13698,13704,13708,13709,13710,13714,13723,13729,13731,13732,13734,13735,13737,13738,13740,13742,13744,13745,13746,13749,13750,13754,13760,13762,13764,13768,13772,13779,13781,13783,13785,13787,13789,13792,13793,13797,13798,13803,13810,13819,13820,13821,13824,13828,13829,13833,13836,13840,13841,13842,13843,13846,13847,13848,13854,13855,13859,13863,13865,13869,13872,13877,13881,13884,13888,13892,13896,13901,13902,13907,13908,13911,13914,13917,13918,13922,13923,13924,13926,13927,13928,13929,13931,13939,13940,13944,13946,13948,13952,13954,13956,13960,13962,13967,13968,13976,13979,13992,13993,13994,13995,14001,14005,14007,14008,14018,14019,14025,14027,14028,14030,14034,14038,14046,14048,14050,14051,14052,14055,14056,14059,14067,14072,14074,14076,14077,14083,14092,14102,14103,14105,14106,14107,14109,14110,14115,14119,14123,14125,14126,14128,14129,14130,14136,14142,14149,14151,14155,14156,14158,14160,14169,14170,14175,14177,14180,14183,14185,14190,14193,14199,14203,14207,14213,14214,14221,14223,14224,14226,14227,14232,14233,14234,14237,14238,14239,14241,14244,14248,14249,14258,14264,14272,14274,14275,14280,14284,14289,14296,14300,14304,14305,14310,14312,14317,14322,14327,14329,14334,14340,14341,14345,14351,14358,14359,14360,14368,14371,14374,14376,14384,14387,14388,14395,14397,14399,14403,14404,14412,14413,14419,14420,14423,14424,14428,14429,14431,14432,14433,14436,14438,14439,14440,14445,14455,14462,14464,14467,14470,14471,14473,14480,14482,14483,14488,14492,14495,14502,14506,14508,14511,14512,14519,14521,14525,14526,14527,14528,14530,14534,14536,14538,14539,14540,14541,14543,14545,14550,14554,14558,14562,14564,14567,14571,14572,14573,14576,14583,14587,14593,14600,14601,14602,14608,14611,14614,14615,14618,14619,14620,14624,14628,14635,14642,14647,14651,14654,14659,14673,14674,14675,14681,14682,14690,14692,14701,14703,14707,14708,14710,14725,14726,14727,14728,14739,14741,14758,14768,14769,14773,14778,14779,14780,14783,14786,14787,14789,14791,14792,14793,14795,14796,14798,14799,14806,14807,14809,14813,14814,14818,14820,14821,14825,14832,14835,14840,14842,14843,14846,14848,14849,14851,14853,14854,14855,14856,14857,14859,14860,14861,14862,14864,14866,14870,14878,14879,14883,14885,14886,14888,14890,14891,14894,14895,14904,14905,14911,14914,14921,14924,14928,14931,14933,14936,14937,14938,14941,14944,14948,14954,14971,14973,14975,14976,14978,14986,14987,14995,15000,15001,15003,15005,15006,15008,15010,15019,15021,15022,15024,15025,15026,15027,15030,15035,15041,15042,15043,15044,15046,15055,15061,15069,15070,15072,15078,15080,15082,15088,15090,15096,15097,15101,15107,15108,15111,15113,15116,15117,15121,15123,15125,15143,15145,15149,15152,15154,15156,15158,15159,15160,15162,15169,15176,15177,15178,15189,15193,15194,15196,15197,15199,15208,15209,15217,15220,15229,15234,15236,15240,15244,15246,15247,15251,15252,15260,15261,15262,15267,15268,15269,15270,15272,15277,15279,15290,15291,15292,15293,15294,15301,15304,15305,15307,15311,15315,15318,15322,15323,15324,15328,15331,15338,15342,15354,15361,15362,15364,15367,15374,15381,15383,15387,15394,15397,15400,15402,15404,15406,15409,15411,15424,15429,15432,15440,15442,15448,15449,15450,15453,15460,15462,15465,15473,15484,15485,15487,15495,15499,15500,15501,15503,15504,15505,15508,15509,15511,15513,15520,15527,15530,15542,15546,15547,15552,15554,15563,15568,15570,15571,15576,15579,15581,15582,15584,15586,15590,15591,15595,15597,15598,15600,15601,15604,15607,15609,15614,15624,15631,15632,15634,15641,15643,15648,15649,15650,15651,15653,15654,15659,15661,15668,15670,15671,15674,15675,15677,15681,15690,15692,15693,15695,15702,15708,15718,15722,15723,15724,15725,15727,15729,15734,15736,15741,15743,15745,15751,15753,15754,15755,15759,15762,15767,15768,15772,15775,15777,15778,15779,15780,15784,15786,15788,15789,15791,15792,15794,15795,15798,15800,15803,15806,15807,15817,15818,15819,15823,15824,15829,15832,15840,15842,15847,15852,15854,15856,15858,15861,15865,15867,15872,15873,15874,15875,15876,15880,15881,15889,15891,15892,15893,15895,15899,15904,15910,15911,15913,15914,15915,15917,15920,15922,15923,15924,15927,15928,15929,15930,15932,15933,15935,15937,15938,15939,15945,15946,15949,15951,15952,15953,15955,15956,15957,15961,15963,15969,15979,15981,15982,15984,15985,15989,15990,15996,16001,16002,16004,16011,16012,16013,16014,16015,16016,16017,16019,16020,16023,16026,16027,16028,16031,16032,16033,16034,16035,16036,16037,16038,16039,16043,16044,16051,16052,16053,16057,16059,16060,16062) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_gcm = pd.read_csv('data/GCM.csv')\n"
     ]
    }
   ],
   "source": [
    "df_diabetes = pd.read_csv('data/diabetes.csv')\n",
    "df_gcm = pd.read_csv('data/GCM.csv')\n",
    "df_japanVoice = pd.read_csv('data/JapaneseVowels.csv')\n",
    "df_wallRobot = pd.read_csv('data/wall-robot-navigation.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">3- Traitement des valeurs nulles/vides</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analysez tous les jeux de données, vérifiez si dans certaines observations il existe des valeurs nulles/vides\n",
    "* Renvoyez le nombre d'obervations qui y sont concernées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We normalize the data by replacing all values that matches \"nan\" or \"null\" case insensitive by Numpy.nan.\n",
    "Then we convert all the columns to be floats.\n",
    "'''\n",
    "\n",
    "def replace_integers(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNan(df, typeOp: int) -> pd.DataFrame:\n",
    "    operations = {\n",
    "        1: lambda x: x.fillna(x.mean()),\n",
    "        2: lambda x: x.fillna(x.max()),\n",
    "        3: lambda x: x.fillna(x.min()),\n",
    "        4: lambda x: x.dropna(),\n",
    "    }\n",
    "    result = operations[typeOp](df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ces observations nécessitent l'une des opérations suivantes:\n",
    "* 1- remplacer les valeurs nulles/vides par la moyenne des valeures des observations pour la caratéristique concernée, à répéter pour chaque  caratéristique.\n",
    "* 2- même opération mais cette-fois ci en calculant le maximum\n",
    "* 3- même opération mais cette-fois ci en calculant le minimum\n",
    "* 4- supprimer les observations contenant les valeurs nulles/vides.\n",
    "\n",
    " Développez une fonction (qu'on appelera processNan) qui prend en paramètres le jeu de données (dataframe) et un entier, qu'on appelera typeOp, compris dans [1,4]\n",
    " En fonction de la valeur de typeOp, vous appliquez l'une des quatre opérations sur le jeu de données fourni en paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = replace_integers(df_diabetes)\n",
    "df_diabetes = processNan(df_diabetes, 1)\n",
    "df_diabetes = read_df_parse_labels(df_diabetes)\n",
    "\n",
    "\n",
    "df_gcm = replace_integers(df_gcm)\n",
    "df_gcm = processNan(df_gcm, 1)\n",
    "df_gcm = read_df_parse_labels(df_gcm)\n",
    "\n",
    "\n",
    "df_japanVoice = replace_integers(df_japanVoice)\n",
    "df_japanVoice = processNan(df_japanVoice, 1)\n",
    "df_japanVoice = read_df_parse_labels(df_japanVoice)\n",
    "\n",
    "df_wallRobot = replace_integers(df_wallRobot)\n",
    "df_wallRobot = processNan(df_wallRobot, 1)\n",
    "df_wallRobot = read_df_parse_labels(df_wallRobot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">4- Traitement des caractéristiques</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Avant de faire appel à un modèle d'apprentissage automatique sur un jeu de données, l'étude de corrélation entre caractéristiques pourrait être menée. Si un couple de caractéristiques ayant une très forte corrélation alors l'une des deux caractéristiques est supprimée.\n",
    "\n",
    "D'après vous, pourquoi on effectue cette suppression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCela nous ferais de la donnée en trop alors qu\\'elle sont \"égaux\" et ne ferais rien d\\'autre que de rajouter une nouvelle caractéristique.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Cela nous ferais de la donnée en trop alors qu'elle sont \"égaux\" et ne ferais rien d'autre que de rajouter une nouvelle caractéristique.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " L'un des moyens pour mesurer la corrélation entre deux caractéristiques est l'utilisation de la mesure de Spearman. cette dernière mesure la force et la direction de la relation n liant les deux caractéristiques.  En ce qui concerne la force de la relation, la valeur du coefficient de corrélation renvoyé par Spearman varie entre +1 et -1.  Une valeur de ± 1 indique un degré de relation parfait entre les deux variables.  Plus la valeur du coefficient de corrélation se rapproche de 0, plus la relation entre les deux variables sera faible.  La direction de la relation est indiquée par le signe du coefficient ; un signe + indique une relation positive et un signe - une relation négative. Si le coefficient est équivant à +1 (repectivement -1) alors il signifie que plus les valeurs d'une caractéristique est grande plus les valeurs de l'autre caractéristique est grande (respectivement petite). \n",
    "\n",
    " Développez une fonction qui retourne, pour une caractéristique donnée, les indices de toutes les caractéristiques dont le coefficient de corrélation retourné par la mesure de Spearman dépasse un certain seuil qu'on appelera t (entré en paramètre). \n",
    "\n",
    "\n",
    "#### Notes: \n",
    "Vous pourriez utiliser la mesure de Spearman proposée par Scipy(https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). Attention, nous nous intéressons seulement au coefficient de corrélation retournée dans la variable corrélation.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import itertools\n",
    "\n",
    "def spearman(df, t: float):\n",
    "    \n",
    "    corr_table = []\n",
    "    \n",
    "    for col1, col2 in itertools.combinations(df.columns, 2):\n",
    "        \n",
    "        # Calcul de la corrélation de Spearman entre les deux colonnes\n",
    "        corr, _ = spearmanr(df[col1], df[col2])\n",
    "        \n",
    "        # Ajout des résultats à la table de corrélation\n",
    "        if corr > t:\n",
    "            index = df.columns.get_loc(col1)\n",
    "            corr_table.append([col1, col2, corr, index])\n",
    "        \n",
    "    return corr_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Pregnancies', 'Glucose', 0.13363229666503892, 0],\n",
       " ['Pregnancies', 'BloodPressure', 0.18512673205801747, 0],\n",
       " ['Pregnancies', 'Age', 0.6072163388236559, 0],\n",
       " ['Glucose', 'BloodPressure', 0.23252741923433431, 1],\n",
       " ['Glucose', 'SkinThickness', 0.04781861047345449, 1],\n",
       " ['Glucose', 'Insulin', 0.1968254606758379, 1],\n",
       " ['Glucose', 'BMI', 0.20582201915080067, 1],\n",
       " ['Glucose', 'DiabetesPedigreeFunction', 0.0630770665695292, 1],\n",
       " ['Glucose', 'Age', 0.27186816841115385, 1],\n",
       " ['BloodPressure', 'SkinThickness', 0.12666506665563632, 2],\n",
       " ['BloodPressure', 'BMI', 0.2929282475431166, 2],\n",
       " ['BloodPressure', 'DiabetesPedigreeFunction', 0.02066116660443163, 2],\n",
       " ['BloodPressure', 'Age', 0.3508945932216354, 2],\n",
       " ['SkinThickness', 'Insulin', 0.54439924155319, 3],\n",
       " ['SkinThickness', 'BMI', 0.4397916797330038, 3],\n",
       " ['SkinThickness', 'DiabetesPedigreeFunction', 0.16581503116350355, 3],\n",
       " ['Insulin', 'BMI', 0.191181741345709, 4],\n",
       " ['Insulin', 'DiabetesPedigreeFunction', 0.204870858154066, 4],\n",
       " ['BMI', 'DiabetesPedigreeFunction', 0.12494245342597647, 5],\n",
       " ['BMI', 'Age', 0.12375757113341947, 5],\n",
       " ['DiabetesPedigreeFunction', 'Age', 0.04717549438983434, 6]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spearman(df_diabetes[0], 0)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Développez une fonction qui supprime d'un jeu de données des caractéristiques à partir de leur indice renseignés en paramètres sous forme de liste. Le but ici est de supprimer les caractéistiques non nécessaires.\n",
    " \n",
    " \n",
    " On peut, supprimer si on le souhaite, les caractériqtiques à partir des noms de colonnes au lieu des indices de colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeIndices(df: pd.DataFrame, indices):\n",
    "  newDf = pd.DataFrame(df)\n",
    "  newDf = newDf.drop(df.columns[indices], axis=1)\n",
    "  return newDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Développez une fonction appelée processCor qui :\n",
    "- parcourt les caractéristiques \n",
    "- et supprime à chaque fois les caractéristiques qui ont \n",
    "un coefficient de corrélation avec la caractéristique courante dépassant le seuil t. \n",
    "\n",
    "Utilisez pour cela les deux fonctions developpées précedement dans la partie 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCor(df: pd.DataFrame, t: float):\n",
    "  tableSpearman = spearman(df, t)\n",
    "  \n",
    "  for column_name in tableSpearman:\n",
    "    df = removeIndices(df, column_name[3])\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148.000000</td>\n",
       "      <td>72</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118.508174</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>35.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>76</td>\n",
       "      <td>48.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>122.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>121.000000</td>\n",
       "      <td>72</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>126.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Glucose  BloodPressure  SkinThickness   BMI  DiabetesPedigreeFunction  \\\n",
       "0    148.000000             72           35.0  33.6                     0.627   \n",
       "1     85.000000             66           29.0  26.6                     0.351   \n",
       "2    118.508174             64            0.0  23.3                     0.672   \n",
       "3     89.000000             66           23.0  28.1                     0.167   \n",
       "4    137.000000             40           35.0  43.1                     2.288   \n",
       "..          ...            ...            ...   ...                       ...   \n",
       "763  101.000000             76           48.0  32.9                     0.171   \n",
       "764  122.000000             70           27.0  36.8                     0.340   \n",
       "765  121.000000             72           23.0  26.2                     0.245   \n",
       "766  126.000000             60            0.0  30.1                     0.349   \n",
       "767   93.000000             70           31.0  30.4                     0.315   \n",
       "\n",
       "     Age  \n",
       "0     50  \n",
       "1     31  \n",
       "2     32  \n",
       "3     21  \n",
       "4     33  \n",
       "..   ...  \n",
       "763   63  \n",
       "764   27  \n",
       "765   30  \n",
       "766   47  \n",
       "767   23  \n",
       "\n",
       "[768 rows x 6 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = processCor(df_diabetes[0], 0.5)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">5- Clustering </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dans cette partie, nous allons associer à un problème (jeu de données) le meilleur modèle de clustering. En conséquence, nous considérons que les jeux de données n'ont pas de labels ou de colonne classe, autrement dit on s'intéresse seulement aux caractéristiques (features). Dans ce cadre, on se focalisera que sur les méthodes de clustering :\n",
    "- Dbscan(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)\n",
    "- Clustering agglomératif(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "- K-means\n",
    "#### Notes:\n",
    "* Un modèle est différent d'un algorithme(méthode). Dans le contexte d'apprentissage automatique,  un algorithme est une procédure exécutée sur un jeu de données. Le modèle est le résultat produit par l'agorithme. \n",
    "* Pour fournir un résultat (modèle), l'algorithme pourrait se baser sur un ensemble d'hyperparamètres qui lui sont fournis en entrée. Un hyperparamètre est une variable qui prend une valeur parmi un ensemble ou une infinité de valeurs, son but est de contrôler le processus d'apprentissage automatique (par exemple, le clustering ou la classification supervisée). \n",
    "* Différentes valeurs des hyperparamètres pourraient aboutir à différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Créez une collection de jeux de données appelée original_datasets. La clé correspond au nom du jeu de données et la valeur correspondante est un tuple (X,y) où X est l'ensemble des caractéristiques et y l'ensemble des labels.  \n",
    "#### Note:\n",
    "Utilisez la fonction développée dans la partie 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"data/diabetes.csv\", \"data/JapaneseVowels.csv\", \"data/wall-robot-navigation.csv\"]\n",
    "original_datasets = []\n",
    "\n",
    "for file in files:\n",
    "  X, y, labels = read_df_parse_labels(pd.read_csv(file), True)\n",
    "  original_datasets.append((X, y, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons préparer les jeux de données avant leur consommation par les méthodes de clustering. Tout d'abord créez une copie de  original_datasets qu'on appelera datasets. Ensuite traitez chaque jeu de données de datasets avec les fonctions processNan et processCor. On fixera typeOp à 1 et t à 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reductionColonnes(data):\n",
    "    data_array = np.array(data)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(data_array)\n",
    "    data_pca = pca.transform(data_array)\n",
    "    return data_pca\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = original_datasets.copy()\n",
    "new_datasets = []\n",
    "#Traitement du datasets, on va nettoyer les dataframes qu'il contient\n",
    "#for i in range(len(datasets)):\n",
    "for i in range(len(datasets)):\n",
    "    data = datasets[i][0].copy()\n",
    "    data = replace_integers(data)\n",
    "    data = processNan(data, 1)\n",
    "    try:\n",
    "        if datasets[i][2] == \"data/GCM.csv\":\n",
    "            data = reductionColonnes(data)\n",
    "            data = replace_integers(data)\n",
    "            data = processNan(data, 1)\n",
    "        data = processCor(data, 0.5)\n",
    "    except:\n",
    "        pass\n",
    "    # Créer un nouveau tuple avec les données modifiées\n",
    "    modified_tuple = (datasets[i][2], data)\n",
    "    \n",
    "    # Ajouter le nouveau tuple à la nouvelle liste\n",
    "    new_datasets.append(modified_tuple)\n",
    "\n",
    "datasets = new_datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la collection datasets, appelez les trois méthodes de clustering sur chaque jeu de données de la collection datasets et évaluez le résultat de  partitionnement(clustering) via:\n",
    "* Calinski Harabaz(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\n",
    "* Davies Bouldin(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_sc)\n",
    "\n",
    "#### Notes:\n",
    "* Sachant qu'on considère que les labels ne sont pas fournis, nous évaluons les résultats aussi par des métriques d'évaluation non supervisées, c'est à dire que les métriques n'ont pas accès aux labels pour proposer un score de performance.\n",
    "* On considère que meilleur est le score des métriques d'évaluation mieux le problème associé au jeu de données est résolu.\n",
    "* Lors de l'appel des méthodes de clustering, dans cette partie on modifiera pas les valeurs des hyperparamètres, on se contentera des valeurs fixés par défaut par scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associez à chaque paire \"jeu de données et méthode de clustering\" les scores retournés par les métriques d'évaluation (calinski, davies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Programme\\Python\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "a:\\Programme\\Python\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "a:\\Programme\\Python\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans\n",
      "[['data/diabetes.csv', 0.7459658305401469, 1143.4415418557362], ['data/JapaneseVowels.csv', 0.5538235805812025, 53448.354100526245], ['data/wall-robot-navigation.csv', 2.16793799856371, 545.4233243759186]]\n",
      "dbscanResult\n",
      "[['data/diabetes.csv', 'Error', 'Error'], ['data/JapaneseVowels.csv', 3.1856743084739416, 6926.532710557646], ['data/wall-robot-navigation.csv', 1.1064579717531862, 8.740773130157462]]\n",
      "aggloclustering\n",
      "[['data/diabetes.csv', 0.7180938560127946, 964.6732488405602], ['data/JapaneseVowels.csv', 0.295726268872482, 8975.34451821781], ['data/wall-robot-navigation.csv', 2.5685867641761138, 784.4396268255854]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "#Initiation des tableaux qui comporte comme première valeur le jeu de donnée, et comme deuxième valeur le score\n",
    "kmeansResult = []\n",
    "dbscanResult = []  \n",
    "aggloclusteringResult = []\n",
    "\n",
    "def evaluateClustering(X, labels):\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        return metrics.calinski_harabasz_score(X, labels), metrics.davies_bouldin_score(X, labels)\n",
    "    else:\n",
    "        return \"Error\", \"Error\"\n",
    "\n",
    "for data in datasets:\n",
    "    #KMEANS \n",
    "    kmeans = KMeans()\n",
    "    labels = kmeans.fit(data[1])\n",
    "    scoreCalinski, scoreDavies = evaluateClustering(data[1], labels.labels_)\n",
    "    newArray = [data[0], scoreDavies, scoreCalinski]\n",
    "    kmeansResult.append(newArray)\n",
    "\n",
    "    #DBSCAN\n",
    "    dbscanClustering = DBSCAN().fit(data[1])\n",
    "    scoreCalinski, scoreDavies = evaluateClustering(data[1], dbscanClustering.labels_)\n",
    "    newArray = [data[0], scoreDavies, scoreCalinski]\n",
    "    dbscanResult.append(newArray)\n",
    "\n",
    "    #AgglomérativeCluster\n",
    "    aggloCluster = AgglomerativeClustering().fit(data[1])\n",
    "    scoreCalinski, scoreDavies = evaluateClustering(data[1], aggloCluster.labels_)\n",
    "    newArray = [data[0], scoreDavies, scoreCalinski]\n",
    "    aggloclusteringResult.append(newArray)\n",
    "\n",
    "\n",
    "#Premier element : nom\n",
    "#Deuxième element : Davies\n",
    "#Troisième element : Calinski\n",
    "print(\"kmeans\")\n",
    "print(kmeansResult)\n",
    "print(\"dbscanResult\")\n",
    "print(dbscanResult)\n",
    "print(\"aggloclustering\")\n",
    "print(aggloclusteringResult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque jeu de données, quelle est la méthode qui a fourni le meilleur modèle selon les scores calculés dans la question précedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour Diabetes.CSV, le meilleur modèle est celui de Aggloclustering ou bien celui de Kmeans. L'un est plus proche de 0, et l'autre a un calinsky plus grand.\n",
    "#Pour japaneseVowel.csv, le meilleur modèle est celui de Aggloclustering.\n",
    "#Et pour robot-navigation.csv, le meilleur modèle est celui dekmeans (dbsScan a un Calunsky trop faible !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">6- Peut-on améliorer encore les résultats de clustering? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous nous intéressons toujours au clustering mais das une perspective d'amélioration des modèles de clustering produits précedement. L'une des pistes d'amélioration des modèles est de varier les valeurs des hyper-paramètres des méthodes de clustering ainsi que les types de pré-traitement appliqués aux jeux de données. \n",
    "\n",
    "Tout d'abord développez une fonction (qu'on appellera getBestModel) qui pour une méthode de clustering donnée et un jeu de données renvoie le meilleur modèle. La fonction prend en paramètre le jeu de données X, le nom de la méthode methodName et une collection d'hyper-paramètres params. Cette collection a pour clé le nom de l'hyper-paramètre et pour valeur la liste des valeurs prises par l'hyper-paramètre. Dans cette fonction, toutes les combinaisons des valeurs des listes dans params seront évaluées et à chaque combinaison on obtiendra un modèle associée au jeu de données et à la méthode de clustering renseignés en paramètre. Chaque modèle est évalué par une métrique d'évaluation au choix (calinski ou davies). \n",
    "\n",
    "La collection params comporte les valeurs possibles des hyper-paramètres de la méthode clustering concernée ainsi que les valeurs possibles de typeOp et t. Vous avez le choix de fixer la liste de valeurs de typeOp et t. Vous pourriez commencer par évaluer [1,2,3,4] pour typeOp et [0.5, 0.7, 0.9] pour t. Les valeurs des hyper-paramètres dépendent de la méthode de clustering ciblée:\n",
    "\n",
    "* Dbscan a plusieurs hyper-paramètres dont les plus influents sont eps, min_samples et metric. Notons que eps est un réel et ne peut dépasser la distance inter-points maximale qui existe dans X, et min_samples est un entier naturel non nul inférieur au nombre d'observations dans X. Metric a par défaut la valeur euclidean, d'autres valeurs sont possibles et consultables sur https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances. Vous avez le choix de fixer les listes de ces trois hyper-paramètres. On vous suggère de ne inclure des valeurs dépassant $\\sqrt{|X|}$ pour min_samples et la moyenne des distances inter-points pour eps.\n",
    "* Clustering agglomératif a pour hyper-paramètres les plus sensibles n_clusters, distance_threshold, affinity(équivalent de metric dans Dbscan) et linkage. A savoir que soit n_clusters ou distance_threshold est utilisé pour extraire les clusters finaux, il n'est pas nécessaire d'utiliser les deux. Vous pourriez limiter la valeur de n_clusters à $\\frac{\\sqrt{|X|}}{2}$ ou limiter la valeur de distance_threshold à la moyenne des distances inter-points.\n",
    "* K-means a deux hyper-paramètres sensibles: n_clusters(nombre de clusters) et l'initialisation des centroides init{‘k-means++’, ‘random’}. \n",
    "\n",
    "#### Note:\n",
    "Vous aurez à instancier plusieurs fois la collection datasets en fonction des valeurs prises par typeOp et t.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestModel(dataSet, methodName, listParam):\n",
    "\n",
    "    tableEval = []\n",
    "    if methodName == \"kmeans\":\n",
    "        for n_cluster, init in itertools.product(listParam[0], listParam[1]):\n",
    "            if isinstance(init, np.ndarray) and init.shape[0] != n_cluster:\n",
    "                continue\n",
    "            kmeans = KMeans(n_clusters=n_cluster, init=init, n_init=1)\n",
    "            labels = kmeans.fit(dataSet)\n",
    "            scoreCalinski, scoreDavies = evaluateClustering(dataSet, labels.labels_)\n",
    "            newArray = [scoreCalinski, \"n_cluster : \" + str(n_cluster), \"init : \" + str(init)]\n",
    "            tableEval.append(newArray)\n",
    "\n",
    "    elif methodName == \"dbscan\":\n",
    "        for eps, min_samples, metric in itertools.product(listParam[0], listParam[1], listParam[2]):\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
    "            labels = dbscan.fit(dataSet)\n",
    "            scoreCalinski, scoreDavies = evaluateClustering(dataSet, labels.labels_)\n",
    "            newArray = [scoreCalinski, \"eps : \" + str(eps), \"min_samples : \" + str(min_samples), \"metric : \" + str(metric)]\n",
    "            tableEval.append(newArray)\n",
    "            \n",
    "    elif methodName == \"agglomeratif\":\n",
    "        for n_cluster, distance_threshold, affinity, linkage in itertools.product(listParam[0], listParam[1], listParam[2], listParam[3]):\n",
    "            if distance_threshold != None:\n",
    "                n_cluster = None\n",
    "            if linkage == \"ward\":\n",
    "                affinity = \"euclidean\"\n",
    "            aggloCluster = AgglomerativeClustering(n_clusters=n_cluster, distance_threshold=distance_threshold, affinity=affinity, linkage=linkage)\n",
    "            labels = aggloCluster.fit(dataSet)\n",
    "            scoreCalinski, scoreDavies = evaluateClustering(dataSet, labels.labels_)\n",
    "            newArray = [scoreCalinski, \"n_cluster : \" + str(n_cluster), \"distance_trhreshold : \" + str(distance_threshold), \"affinity : \" + str(affinity), \"linkage : \" + str(linkage)]\n",
    "            tableEval.append(newArray)\n",
    "\n",
    "    return methodName, tableEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction getBestModel, identifiez le meilleur modèle associé à chaque jeu de données et méthode de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans\n",
      "[['Error', 'n_cluster : 1', 'init : k-means++'], ['Error', 'n_cluster : 1', 'init : random'], [1000.553607261077, 'n_cluster : 2', 'init : k-means++'], [1000.553607261077, 'n_cluster : 2', 'init : random'], [1180.5761599435994, 'n_cluster : 3', 'init : k-means++'], [1180.9887947381244, 'n_cluster : 3', 'init : random'], [1180.4875035227085, 'n_cluster : 3', 'init : [[1 2 3 4 5 6 7]\\n [2 2 3 3 4 5 6]\\n [1 2 3 4 5 6 7]]']]\n"
     ]
    }
   ],
   "source": [
    "diabetes = datasets[0][1]\n",
    "#Format : cluster / init \n",
    "paramDiabetes = [[1,2,3], [\"k-means++\", \"random\", np.array([[1, 2, 3, 4, 5, 6, 7], [2, 2, 3, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6, 7]])]]\n",
    "\n",
    "\n",
    "\n",
    "nameMethode, diabetesResult = getBestModel(diabetes, \"kmeans\", paramDiabetes)\n",
    "print(nameMethode)\n",
    "print(diabetesResult)\n",
    "nameMethode, diabetesResult = getBestModel(diabetes, \"kmeans\", paramDiabetes)\n",
    "\n",
    "japan = datasets[1][1]\n",
    "#Format : eps, min_samples, metrics\n",
    "paramJapan = [[0.5, 1.5, 3], [5, 10, 15], [\"euclidean\", \"manhattan\", \"cityblock\"]]\n",
    "\n",
    "nameMethode, japanResult = getBestModel(japan, \"dbscan\", paramJapan)\n",
    "print(nameMethode)\n",
    "print(japanResult)\n",
    "\n",
    "\n",
    "robotNavigation = datasets[2][1]\n",
    "#Format : n_clusters, distance_threshold, affinity(équivalent de metric dans Dbscan) et linkage\n",
    "paramRobot = [[2,3,4], [None, 2, 3], [\"euclidean\", \"manhattan\", \"cosine\"], ['ward', 'complete', 'average']]\n",
    "nameMethode, robotResult = getBestModel(robotNavigation, \"agglomeratif\", paramRobot)\n",
    "print(nameMethode)\n",
    "print(robotResult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par rapport à la partie 5, y a t-il eu des améliorations? Justifiez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\Programme\\Python\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">7- La classification supervisée</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous chercherons à résoudre les problèmes par la classification supervisée. Dans ce cas, on considère que les labels (y) sont à notre disposition. Nous nous intéressons à trois méthodes de cette catégorie:\n",
    "* arbre de décision (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "* KNN : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancier la collection datasets dans les mêmes conditions que celles spécifiées dans la partie 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur chaque jeu de données de la collection datasets appliquez les deux méthodes de classification. A chaque application, évaluez le résultat (modèle) via les métriques d'évaluation suivantes:\n",
    "* accuracy score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "* balanced accuracy score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score)\n",
    "\n",
    "#### Notes.\n",
    "* A l'appel des méthodes de classification, les valeurs des hyper-paramètres doivent restés fixés tel qu'elles sont par scikit-learn. \n",
    "* Avant d'appliquer les trois méthodes de classification sur le jeu de données, celui-ci doit être divisé en deux sous-ensembles: ensemble d'entraînement ($~80$% du jeu de données) et l'ensemble de test ($~20$%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la méthode ayant a fourni le meilleur modèle (c'est à dire le meilleur taux de prédiction) pour chaque jeu de données?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">8- Peut-on encore améliorer les résultats de classification supervisée</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même perspective que dans la partie 6, nous chercherons ici à améliorer les résultats de la classification en faisant varier les valeurs des hyper-paramètres et les types de pré-traitement des données. \n",
    "\n",
    "Reprenez la méthode gestBestModel en faisant en copie de celle-ci et en la nommant getBestModelC. Cette dernière a la même fonction que gestBestModel sauf qu'elle est adaptée pour la classification supervisée: \n",
    "* la métrique d'évaluation doit correspondre à l'une des deux métriques de la partie 7. \n",
    "* Pour faire face aux problèmes de désiquilibre de répartition de classes de points dans les sous-ensembles d'entraînement et de test, on adoptera la technique d'échantillonage suivante. A partir de chaque jeu de données, on génére 5 sous-ensembles de données disjoints. A chaque itération, un sous-ensemble est utilisé pour le test et les autres pour la phase d'entraînement. Un sous-ensemble doit être utilisé une seule fois dans la phase de test. \n",
    "\n",
    "Les listes de valeurs de typeOp et t peuvent être fixées identiquement à celles de la partie 6. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction getBestModelC, identifiez le meilleur modèle associé à chaque jeu de données et méthode de classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par rapport à la partie 7, y a t-il eu des améliorations? Justifiez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
